{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Chapter 04.05 학습 알고리즘 구현하기**"
      ],
      "metadata": {
        "id": "SNNPf8Tt8P7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***신경망 학습 절차***\n",
        "\n",
        "0. **전제**\n",
        "  * 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 ***학습***이라고 한다.\n",
        "1.   **미니배치**\n",
        "  * 훈련 데이터 중 일부를 무작위로 가져오기 ➜ 미니배치\n",
        "   * 미니배치의 손실 함수 값 줄이는 것이 목표\n",
        "2.   **기울기 산출**\n",
        " * 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기 구함\n",
        " * 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시\n",
        "3.   **매개변수 갱신**\n",
        " * 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
        "4.   **반복**\n",
        "   * 1~3단계 반복\n"
      ],
      "metadata": {
        "id": "FF9wmt_c8m9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***확률적 경사 하강법 (SGD : Stochastic Gradient Descent)***\n",
        "\n",
        "* 확률적으로 무작위로 골라낸 데이터 (미니배치)에 대해 수행하는 경사 하강법"
      ],
      "metadata": {
        "id": "AxcWDZvOQKQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"35%\" src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcGGhcs%2FbtqYAsMiu5M%2FuzRZ22SMGALicF6VveFjjk%2Fimg.png\">\n",
        "\n",
        "<img width=\"50%\" src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FUS7rW%2FbtqYDj9a5Qc%2F78VFb2QlmeVUwkp0N1RH1k%2Fimg.png\">"
      ],
      "metadata": {
        "id": "XwDW6EyVHkzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.5.1 2층 신경망 클래스 구현 (은닉층이 1개인 네트워크)**"
      ],
      "metadata": {
        "id": "7WtdN1AM8cej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from functions import *\n",
        "from gradient import numerical_gradient\n",
        "\n",
        "class TwoLayerNet:\n",
        "  # 가중치 초기화 : 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "    # 신경망의 가중치 매개변수를 보관하는 딕셔너리 변수\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  # 예측(추론) 실행\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y\n",
        "\n",
        "  # 손실 함수 값 계산 (x : 입력 데이터, t : 정답 레이블)\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  # 정확도 계산\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis = 1)\n",
        "    t = np.argmax(t, axis = 1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "\n",
        "  # 수치 미분 방식으로 각 매개변수 기울기 계산 (x : 입력 데이터, t : 정답 레이블)\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W : self.loss(x, t)\n",
        "\n",
        "   # 기울기를 보관하는 딕셔너리 변수\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "  # 오차역전파법 사용하여 기울기 계산\n",
        "  def gradient(self, x, t):\n",
        "    return 0"
      ],
      "metadata": {
        "id": "CpelNWteCmsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.5.2 미니배치 학습 구현**"
      ],
      "metadata": {
        "id": "RDZISawydxjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28**2)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "## 하이퍼파라미터\n",
        "# 반복 횟수\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "# 미니배치 크기\n",
        "batch_size = 100\n",
        "# 학습률\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # 미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 기울기 계산\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "  # 매개변수 갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # 학습 경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)"
      ],
      "metadata": {
        "id": "M2ylO7Z6RozH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.5.3 시험 데이터로 평가**"
      ],
      "metadata": {
        "id": "0u4NlUhCd2rQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***에폭 (epoch)***\n",
        "\n",
        "* 1epoch 이란 전체 훈련 데이터셋에 대해 forward pass와 backward pass 과정을 모두 포함하여 전체적으로 한 번 학습했을 때를 말함\n",
        "  * 예) 전체 훈련 데이터셋에 대해 50번 학습을 하면 epochs = 50\n",
        "  * 예) 훈련 데이터 수 = 1000, epochs = 10, batch_size = 200\n",
        "       * iteraction (데이터를 나누는 회수) = 1000 / 200 = 5\n",
        "       * 즉, 1000개의 데이터를 200개로 나누어 5번 학습하는 것을 총 10번 (10 epochs) 반복"
      ],
      "metadata": {
        "id": "QILQcWy0JGyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28**2)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "## 하이퍼파라미터\n",
        "# 반복 횟수\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "# 미니배치 크기\n",
        "batch_size = 100\n",
        "# 학습률\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # 미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 기울기 계산\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "  # 매개변수 갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # 학습 경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  # 1에폭당 정확도 계산\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "    print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "metadata": {
        "id": "Bx5CEjwkfUF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}