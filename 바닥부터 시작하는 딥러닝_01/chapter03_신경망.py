# -*- coding: utf-8 -*-
"""Chapter03. 신경망.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGdfiLDKov2c2ehfZSrtOn8gT9V1Lxvr

### **Chapter03. 신경망 (Neural Network)**

####**3.2 활성화 함수**

**[활성화 함수 식]**

$x = b + w_{1}x_{1} + w_{2}x_{2}$

$h(x) =\begin{cases}0 & (x \leq  0)  \\1 & (x >  0) \end{cases} $

$y = h(a)$
"""

## 신경망 구조 : 입력층, 은닉층, 출력층
## 단순 퍼셉트론 : 단층 네트워크에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델
## 다층 퍼셉트론 : 신경망 (여러 층으로 구성되고, 시그모이드 함수 등 매끈한 활성화 함수를 사용하는 네트워크)
### 활성화 함수 (Activation Function) : 입력 신호의 총합을 출력 신호로 변환하는 함수/입력 신호의 총합이 활성화를 일으키는지 정하는 역할

## 계단 함수 구현 : 임계값을 경계로 출력이 바뀌는 함수 ⟹ 선형 함수
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
  return np.array(x > 0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()

"""**[시그모이드 함수 식]**

$h(x) = \frac{1}{1 + exp(-x)} $

$exp(-x) \Longrightarrow  e^{-x} $
"""

## 시그모이드 함수 (Sigmoid Function) : s자 모양 함수 ⟹ 비선형 함수
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  return 1 /  (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()

"""**[ReLU 함수 식]**

$h(x) =\begin{cases}x & (x >  0)  \\0 & (x \leq  0) \end{cases} $
"""

## ReLU 함수 (Rectified Linear Unit : 렐루) : 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0 출력 ⟹ 비선형 함수
def relu(x):
  return np.maximum(0, x)

"""####**3.3 다차원 배열의 계산**"""

a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
print(a)

# 배열의 차원 수 확인
np.ndim(a)

# 배열의 형상 확인
a.shape

# 행렬의 스칼라곱
a = np.array([[1, 2], [3, 4], [5, 6]])
b = np.array([[5, 6], [7, 8]])
np.dot(a, b)

"""#### **3.4 신경망 구현**"""

## 각 층의 신호 전달 구현
# 0층에서 1층 신호 전달
x = np.array([1.0, 0.5])
w1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
b1 = np.array([0.1, 0.2, 0.3])

a1 = np.dot(x, w1) + b1
a1

z1 = sigmoid(a1)
z1

# 1층에서 2층 신호 전달
w2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
b2 = np.array([0.1, 0.2])

a2 = np.dot(z1, w2) + b2
z2 = sigmoid(a2)
z2

## 향등 함수 : 입력을 그대로 출력하는 함수 ⟹ 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 설정
def identity_Function(x):
  return x

# 2층에서 출력층으로 신호 전달
w3 = np.array([[0.1, 0.3], [0.2, 0.4]])
b3 = np.array([0.1, 0.2])

a3 = np.dot(z2, w3) + b3
y = identity_Function(a3)
y

## 3층 신경망 순방향 구현 정리
# 가중치와 편향 초기화하고, 딕셔너리 형태로 저장
def init_network():
  network = {}
  network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
  network['b1'] = np.array([0.1, 0.2, 0.3])
  network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
  network['b2'] = np.array([0.1, 0.2])
  network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
  network['b3'] = np.array([0.1, 0.2])

  return network

# 입력 신호 출력 신호로 변환
def forward(network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, W3) + b3
  y = identity_Function(a3)

  return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)

"""#### **3.5 출력층 설계**"""

### 기계 학습 문제는 크게 분류와 회귀로 나눔
## 분류 : 데이터가 어느 클래스에 속하느냐는 문제 ⟹ 소프트맥스 함수 사용
## 회귀 : 입력 데이터에서 연속적인 수치를 예측하는 문제 ⟹ 향등 함수 사용

"""**[소프트맥스 함수 식]**

$y_{k} = \frac{exp(a_{k})}{\sum exp(a_{i})}$
"""

## 소프트맥스 함수 : 모든 입력 신호로부터 화살표를 받음 (출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 떄문)
a = np.array([0.3, 2.9, 4.0])

# 지수 함수
exp_a = np.exp(a)
print(exp_a)

# 지수 함수의 합
sum_exp_a = np.sum(exp_a)
print(sum_exp_a)

y = exp_a / sum_exp_a
print(y)

# 소프트맥스 함수 구현
def softmax(a):
  exp_a = np.exp(a)
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

# 소프트맥스 함수 오버플로 문제 해결
def sofmax(a):
  C = np.Max(a)
  exp_a = np.exp(a - C) # overflow 대책
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

# 소프트맥스 함수 중요 성질 : 출력이 0~1.0 사이의 실수이며, 출력의 총합은 1이다 ⟹ 문제를 확률적(통계적)으로 대응 가능
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
np.sum(y)

"""####**3.6 손글씨 숫자 인식**

MNIST 모델 참고 : https://my-brain.tistory.com/4
"""

## 신경망의 순전파 (Forward Propagation) : 신경망의 추론 과정
# MNIST 데이터셋 : 손글씨 숫자 이미지 집합
from keras.datasets import mnist

(x_train, t_train), (x_test, t_test) = mnist.load_data()

img = x_train[0]
label = t_train[0]
print(label)
print(img.shape)

from matplotlib.pyplot import imshow
#imshow(img.reshape(28, 28))
imshow(x_train[0])

## 신경망의 추론 처리 ⟹ 입력층 뉴런 : 784개 (28X28) / 출력층 뉴런 : 10개
from keras.datasets import mnist
import numpy as np
import pickle

def get_data():
  (x_train, t_train), (x_test, t_test) = mnist.load_data()
  return x_test, t_test

# pkl 파일 불러오기 : 가중치 & 편향 수치
def init_network():
  with open("/content/sample_weight.pkl", "rb") as f:
            network = pickle.load(f)

  return network

# 각 레이블의 확률을 넘파이 배열로 반환
def predict(network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, W3) + b3
  y = softmax(a3)

  return y

x, t = get_data()
x = x.reshape(-1, 28**2)
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
  y = predict(network, x[i])
  p = np.argmax(y)  # 확률이 가장 높은 원소의 인덱스

  if p == t[i]:
    accuracy_cnt += 1

print("Accuracy : " + str(float(accuracy_cnt) / len(x)))

## 정규화 (Normalization) : 데이터를 특정 범위로 변환하는 처리
## 전처리 (Pre-processing) : 신경망의 입력 데이터에 특정 변환을 가하는 것
## 데이터 백색화 : 전체 데이터를 균일하게 분포시키는 작업

## 배치 (batch) 처리 : 하나로 묶은 입력 데이터
x, t = get_data()
x = x.reshape(-1, 28**2)
network = init_network()

batch_size = 100
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
  x_batch = x[i:i+batch_size]
  y_batch = predict(network, x_batch)
  p = np.argmax(y_batch, axis=1)
  accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy : " + str(float(accuracy_cnt) / len(x)))